---
title: "Tema 2.3. Series de tiempo para pronóstico"
subtitle: "Curso: Decisiones Estadísticas y Administrativas"
author: "Prof. Shu Wei Chou Chen"
institute: "Escuela de Estadística, UCR"
date: ""
output:
  xaringan::moon_reader:
    css: [default,  metropolis, metropolis-fonts, ninjutsu,"text_color.css"]
  html_document: default
  pdf_document: default
fontsize: 12pt
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE,fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%")
```


```{r, include=FALSE}
library(ggplot2)
library(forecast)
library(fpp2)
library(astsa)
library(car)
```

# Subtemas

1. Series de tiempo y pronóstico
2. Descomposición de series temporales
3. **Técnicas de suavizamiento exponencial**
4. **Regresión aplicada a series cronológicas**


---
# Contenido

1. Introducción
2. Suavizamiento exponencial simple
3. Método lineal de Holt
4. Métodos multiplicativo y aditivo de Holt-Winters


---
# Introducción

- Desarrollados en los años 1950s.
- Son aplicados ampliamente debido a su sencillez y bajo costo.
- Pueden ser utilizados para pocas observaciones.

1. Suavizamiento exponencial simple (SES)
2. Método lineal de Holt (con tendencia)
3. Métodos multiplicativo y aditivo de Holt-Winters (con estacionalidad)

---
# Suavizamiento exponencial simple (SES)

- Es apropiada para serie que no tienen patrones estacionales ni de tendencia, y cuya la media o nivel cambia lentamente.

- Notación:
  - $Z_t$: la observación de la serie en el tiempo $t$.
  - $P_t$: el pronóstico del tiempo $t$.
  - $Z_t-P_t$: el error de pronóstico.

- El método de SES consiste en:
$$P_{t+1}=P_t+\alpha (Z_t-P_t)$$
donde $0<\alpha<1$ es el parámetro de suavizamiento.<br />
i.e., el pronóstico en el tiempo $t+1$ es una combinación del pronóstico en el tiempo $t$ y una proporción $\alpha$ el error de pronóstico del tiempo $t$.


---
# Suavizamiento exponencial simple (SES)

- Note que la ecuación anterior es equivalente a:
$$P_{t+1}=\alpha Z_t + (1-\alpha)~ P_t$$
i.e., el pronóstico en el tiempo $t+1$ es un promedio ponderado de la observación más reciente y el pronóstico en el tiempo $t$.

- Recursivamente se obtienen:

$$P_{t+1}=\alpha Z_t + (1-\alpha) \left[ \alpha Z_{t-1} + (1-\alpha) P_{t-1} \right]$$
$$=\alpha Z_t + \alpha (1-\alpha) Z_{t-1} + (1-\alpha)^2 P_{t-1}$$
$$=\alpha Z_t + \alpha (1-\alpha) Z_{t-1}+\alpha (1-\alpha)^2 Z_{t-2}+... + \alpha (1-\alpha)^{t-1} Z_1 + (1-\alpha)^t P_{1}$$
- Los coeficientes $\alpha, \alpha (1-\alpha)^2,..., \alpha (1-\alpha)^t$ decrecen exponencialmente.
- El pronóstico $P_{t+1}$ es un promedio ponderado de las observaciones pasadas $Z_t,...,Z_1$ ya que  $(1-\alpha)^t P_{1}$ es casi nulo.

---
# Suavizamiento exponencial simple (SES)

- La idea es realizar SES con diferentes valores de $\alpha$ y seleccionar el valor de $\alpha$ que minimiza la suma de los cuadrados de los errores de pronóstico, o el MSE.
$$MSE=\frac{\sum\limits_{t=1}^T \left( Z_t-P_t \right)^2}{T} $$

---
# Suavizamiento exponencial simple (SES)

- Ejemplo 3.1 de Hernández (2011): Serie mensual de defunciones de Costa Rica de los años 2001 y 2002.

```{r echo=FALSE, out.width = "35%", fig.align="center"}
defunciones<-read.csv("defunciones.csv",sep=",")
y<-ts(defunciones$defunciones,start=c(2001,1),frequency=12)
forecast::autoplot(y) 
```

---
# Suavizamiento exponencial simple (SES)


```{r out.width = "35%", fig.align="center"}
ses1 <- ses(y)
ses1$model
```

---
# Suavizamiento exponencial simple (SES)


```{r out.width = "50%", fig.align="center"}
autoplot(ses1) +
  autolayer(fitted(ses1), series="ajustado") +
  ylab("defunciones") + xlab("mes")
```


---
# El método lineal de Holt

- El método de Holt sirve para series con tendencia.
- El suavizamiento exponencial de Holt utiliza 3 ecuaciones y dos parámetros ( $\alpha$ y $\beta$ ):

Ecuación del nivel: $~~~~~~~~~~~~~l_{t}=\alpha Z_{t} + (1-\alpha)~ (l_{t-1}+b_{t-1})$

Ecuación de la pendiente: $~~~~b_{t}=\beta (l_t-l_{t-1}) + (1-\beta)~ b_{t-1}$

Ecuación del pronóstico: $~~~~~P_{t+m}=l_t+b_t m$
- $l_t$ es una estimación del nivel promedio de $Z_t$
  - es un promedio ponderado del valor de $Z_t$ y una estimación del nivel de la serie en $t$.
- $b_t$ es una estimación de la pendiente de $Z_t$.
  - es un promedio ponderado del aumento del nivel de la serie entre $t$ y $t-1$, y una estimación de la pendiente en el tiempo $t-1$.
- La última ecuación pronostica el valor de $Z_{t+m}$, i.e., pronóstico a $m$ paso para adelante.

- Es conocido como **suavizamiento exponencial doble**.

---
# El método lineal de Holt

- Al igual que SES, los valores de $\alpha$ y $\beta$ se obtienen minimizando la suma de los cuadrados de los errores de pronóstico, o el MSE.
$$MSE=\frac{\sum\limits_{i=1}^T \left( Z_t-P_t \right)^2}{T} $$

- Ejemplo 3.2 de Hernández (2011): Serie de graduados del ITCR de 1975-2002.

```{r echo=FALSE, out.width = "35%", fig.align="center"}
itcrgrad<-read.csv("ITCR.csv",sep=",")
y<-ts(itcrgrad$graduados,start=1975)
autoplot(y) 
```



---
# El método lineal de Holt

```{r out.width = "35%", fig.align="center"}
holt2 <- holt(y , h=5)
holt2$model
```

---
# El método lineal de Holt

```{r out.width = "50%", fig.align="center"}
autoplot(holt2) +
  autolayer(fitted(holt2), series="ajustado") +
  ylab("defunciones") + xlab("mes")
```


---
# El método multip. y adit. de H-W

- [Winters (1960)](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.6.3.324) extendió el método lineal de Holt para tomar en cuenta la estacionalidad, el cual es conocido como Holt-Winters.

1. **Método multiplicativo**
  - Variación estacional cambia proporcionalmente con el nivel de la serie.

2. **Método aditivo**
  - Variación constante a lo largo de tiempo.

---
# El método multiplicativo de H-W


$$l_t= \alpha \frac{Z_t}{S_{t-s}}+(1-\alpha) (l_{t-1}+b_{t-1})$$
$$b_t= \beta (l_t-l_{t-1})+(1-\beta) b_{t-1}$$
$$S_t= \gamma \frac{Z_t}{l_{t-1}+b_{t-1}}+(1-\gamma) S_{t-s}$$
$$P_{t+m}= (l_{t}+b_t~m) S_{t+m-s}$$
donde <br /> 
$s$ es la longitud de la estacionalidad, <br /> $l_t$ es el nivel de la serie $Z_t$, <br /> $b_t$ es la tendencia, <br /> $S_t$ es el componente estacional, <br /> $P_{t+m}$ es el pronóstico $m$ pasos adelante y <br /> $0<\alpha<1$, $0<\beta<1$, $0<\gamma<1$.

---
# El método multiplicativo de Holt-Winters

- Ejemplo 3.3 de Hernández (2011): Serie mensual de turistas de 1991-2000.

```{r echo=FALSE, out.width = "35%", fig.align="center"}
turistas<-read.csv("turistas.csv",sep=";")
y<-ts(turistas$turistas,start=c(1991,1),frequency=12)
autoplot(y) 
```


---
# El método multiplicativo de H-W

```{r echo=FALSE,out.height = '400px', out.width = '600px', fig.align="center"}
ht1 <- hw(y,seasonal="multiplicative")
autoplot(y) +
  autolayer(ht1, series="HW multiplicativo", PI=FALSE, size = 1) +
  xlab("año") +
  ylab("turistas") +
  ggtitle("Turistas que ingresaron a CR") +
  guides(colour=guide_legend(title="pronóstico"))
```


---
# El método aditivo de Holt-Winters


$$l_t= \alpha (Z_t-S_{t-s})+(1-\alpha) (l_{t-1}+b_{t-1})$$
$$b_t= \beta (l_t-l_{t-1})+(1-\beta) b_{t-1}$$
$$S_t= \gamma \left( Z_t-l_{t-1}-b_{t-1} \right)+(1-\gamma) S_{t-s}$$
$$P_{t+m}= l_{t}+b_t~m + S_{t+m-s}$$
donde <br /> 
$s$ es la longitud de la estacionalidad, <br /> $l_t$ es el nivel de la serie $Z_t$, <br /> $b_t$ es la tendencia, <br /> $S_t$ es el componente estacional, <br /> $P_{t+m}$ es el pronóstico $m$ pasos adelante y <br /> $0<\alpha<1$, $0<\beta<1$, $0<\gamma<1$.

---
# El método aditivo y mult. de Holt-Winters


```{r echo=FALSE,out.height = '300px', out.width = '500px', fig.align = "center"}
ht2 <- hw(y,seasonal="additive")


autoplot(y) +
  autolayer(ht1, series="HW multiplicativo", PI=FALSE, size = 1) +
  autolayer(ht2, series="HW aditivo", PI=FALSE, size = 1) +
  xlab("año") +
  ylab("turistas") +
  ggtitle("Turistas que ingresaron a CR") +
  guides(colour=guide_legend(title="pronóstico"))
```


RMSE del método multiplicativo: 3239.92.  
RMSE del método aditivo: 4297.824.   

---
# Regresión aplicada a series cronológicas

- La idea es ajustar un modelo de regresión para la serie temporal $Y_t, t=1,...,T$ utlizando un conjunto de $p$ covariables: $X_1,...,X_p$.

- Por ejemplo: 
  - $Y$ una serie mensual de ventas con la variable independiente $X_1$ gasto mensual en anuncios.
  - $Y$ una serie diaria de demanda de energía eléctrica con las variables independientes: $X_1$ temperatura y $X_2$ el día de la semana.


---
# Regresión lineal múltiple

- La forma general de un modelo de regresión lineal múltiple es:

$$Y_t=\beta_0+\beta_1 X_{t,1}+\beta_2 X_{t,2}+...+\beta_p X_{t,p}+\epsilon_t, t=1,...,T,$$ 

donde $Y_t$ es la variable a pronosticar y $X_1,...,X_p$ son los $p$ variables predictoras. Las variables predictoras pueden ser numéricas o categóricas (con el manejo apropiado de factores).   
Los coeficientes $\beta_1,...,\beta_p$ miden el efecto de cada predictor después de tener en cuenta los efectos de todos los demás predictores del modelo. Por lo tanto, los coeficientes miden los efectos marginales de las variables predictoras.


---
# Regresión lineal múltiple

- El modelo de regresión lineal múltiple en su forma matricial:

$$
Y=X \beta+\epsilon
$$
donde

$$Y=\left[ \begin{array}{c}Y_1 \\ \vdots \\Y_T \end{array}  \right],~~ X= \left(\begin{array}{ccccc} 1& X_{11}& X_{12} & ... & X_{1p}\\ 1 &  X_{21}& X_{22} & ... &X_{2p}\\ \vdots& \vdots & \ddots &\vdots& \vdots\\ 1&  X_{T1}& X_{T2} & ... &X_{Tp} \end{array}\right),$$
$$\beta=\left[ \begin{array}{c}\beta_0 \\ \vdots \\\beta_T \end{array}  \right],~~\epsilon=\left[ \begin{array}{c}\epsilon_1 \\ \vdots \\\epsilon_T \end{array}  \right].$$ 

---
# Regresión lineal múltiple


**Supocisiones del modelo:**

- La relación entre la variable de pronóstico y las variables predictoras satisface esta ecuación lineal.

- Los errores $\varepsilon_1,...,\varepsilon_T$:
  - tienen media cero,
  - no están autocorrelacionados,
  - no están relacionados con las variables predictoras

- Los errores se distribuyan normalmente con una varianza constante $\sigma^2$.

- Cada predictor $X_i, i=1,...,p$ supone que es observado y fijo, i.e. no es una variable aleatoria.

---
# Regresión lineal múltiple

**Tópicos importantes:**

1. Estimación:
  - por mínimos cuadrados.
  - por máxima verosimilitud.
  
2. Selección de variables

3. Diagnósticos

4. Medidas remediales

---
# Regresión lineal múltiple: Ejemplo 

Se tienen datos de variaciones porcentuales trimestrales (tasas de crecimiento) del consumo personal real (Y), ingresos, producción industrial, ahorros personales y tasa de desempleo, para EE.UU. desde 1970 a 2016.


```{r MultiPredictors, out.width = "60%", fig.align="center", echo=FALSE}

autoplot(uschange[,1:5], facets = TRUE, colour=TRUE) +
  ylab("") + xlab("Year") +
  guides(colour="none")
```


---
# Regresión lineal múltiple: Ejemplo

```{r ScatterMatrix, out.width = "50%", fig.align="center", echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Matriz de diagrama de dispersión del gasto de consumo de EE. UU. y los cuatro predictores."}
uschange %>%
  as.data.frame() %>%
  GGally::ggpairs()
```


---
# Regresión lineal múltiple: Ejemplo

```{r tslm_mult, echo=FALSE}
fit.consMR <- tslm(
  Consumption ~ Income + Production + Unemployment + Savings,
  data=uschange)
summary(fit.consMR)
```


---
# Regresión lineal múltiple: Ejemplo

```{r, out.width = "60%", fig.align="center", echo=FALSE}
autoplot(uschange[,'Consumption'], series="Data") +
  forecast::autolayer(fitted(fit.consMR), series="Fitted") +
  xlab("Year") + ylab("") +
  ggtitle("Cambio porcentual de gastos de consumo en EE.UUU ") +
  guides(colour=guide_legend(title=" "))

```


---
# Regresión lineal múltiple: Ejemplo

```{r, out.width = "50%", fig.align="center", echo=FALSE}
checkresiduals(fit.consMR)

```


---
# Regresión espuria

- Cuando las series no son estacionarias, los resultados de regresión no son confiables y presenta lo que se llama correlación espuria.

- Los datos de las series cronológicas de tendencias pueden parecer relacionados.
- Por ejemplo, los pasajeros aéreos en Australia tienen una correlación positiva con la producción de arroz en Guinea.

```{r spurious, echo=FALSE, out.height = '250px', out.width = '350px', fig.align="center", fig.asp=0.6, warning=FALSE, message=FALSE}
aussies <- window(ausair, end=2011)
p1 <- autoplot(aussies) + xlab("Year") + ylab("Air Passengers in Australia")
p2 <- autoplot(guinearice) + xlab("Year") + ylab("Rice Production in Guinea")
p3 <- cbind(guinearice, aussies) %>%
  as.data.frame() %>%
  ggplot(aes(x=guinearice, y=aussies)) +
    geom_point() +
    ylab("Air Passengers in Australia (millions)") +
    xlab("Rice Production in Guinea (million tons)")
gridExtra::grid.arrange(gridExtra::arrangeGrob(p1, p2),p3, ncol = 2)
```

---
# Regresión espuria

```{r tslm_mult1, echo=FALSE}
fit <- tslm(aussies ~ guinearice)
summary(fit)
```

---
# Regresión espuria

```{r tslm_mult2, out.height = '300px', out.width = '300px',fig.align="center", echo=FALSE}
checkresiduals(fit)
```

---
# Modelos de tendencia

- Como las variables independiente son asumidas como fijas, se puede utilizar el tiempo como una variable independiente.

- Los modelos más frecuentes:
  - **Tendencia lineal**:
  $$Y_t=\beta_0+\beta_1 t + \epsilon_t$$
  - **Tendencia cuadrática**:
  $$Y_t=\beta_0+\beta_1 t +\beta_2 t^2 + \epsilon_t$$

- Regresión no lineal.
  - Por ejemplo: LOESS.

---
# Modelos de tendencia

Vamos a ajustar un modelo de tendencia cuadrática a la serie de graduados de ITCR de 1975 a 2002:
  $$Y_t=\beta_0+\beta_1 t +\beta_2 t^2 + \epsilon_t, t=1,...,T$$

```{r echo=FALSE, out.width = "45%", fig.align="center"}
itcrgrad<-read.csv("ITCR.csv",sep=",")
y<-ts(itcrgrad$graduados,start=1975)
tiempo<-seq(1,length(y))
tiempo2<-tiempo^2
mod<-lm(y~tiempo+tiempo2)

autoplot(y) +
  ylab("ITCR") +
  autolayer(ts(mod$fitted.values,start=1975), series = "ajustado") 

```


---

# Modelos de tendencia


```{r comment=NA, out.width = "35%", fig.align="center"}
tiempo<-seq(1,length(y))
tiempo2<-tiempo^2
mod<-lm(y~tiempo+tiempo2)
summary(mod)
```


---
# Modelos de tendencia

.pull-left[
```{r echo=FALSE}
hist(mod$residuals,main="",xlab="residuales")
```
]

.pull-right[
```{r echo=FALSE}
ts.plot(mod$residuals,ylab="residuales") 
```
]

---
# Modelos de tendencia

- Supuestos:
  - Normalidad
  - Homoscedasticidad
  - Autocorrelación en el tiempo

```{r echo=FALSE, out.width = "35%", fig.align="center"}
e<-mod$residuals
lag1.plot(e)
```


---
# Modelos de tendencia

**Prueba de Durbin-Watson:**
Dado un modelo de regresión ajustado, suponga que el tipo de autocorrelación entre los errores tiene orden 1, i.e.
$$\epsilon_t= \rho \epsilon_{t-1}+a_t$$
donde $\rho$ es la correlación de un rezago y $a_t \sim N(0,\sigma^2)$

Defina las hipótesis:<br /> 
$H_0: \rho=0$  <br /> 
$H_1: \rho \neq 0$ <br /> 


```{r comment=""}
durbinWatsonTest(mod)
```

---
# Modelos de tendencia


```{r, out.width = "50%", fig.align="center", echo=FALSE}
checkresiduals(mod)

```


---
# Transformaciones en modelos estacionales

- Si la magnitud del cambio estacional se mantiene aproximadamente constante con el cambio del nivel de la serie, se dice que la variación estacional es constante (aditiva).
- Si la variación estacional aumenta proporcionalmente con el nivel de la serie, se dice que la variación estacional es multiplicativa.

Transformaciones usadas en la práctica:

- $W_t=Y_t^\alpha,~~\text{con} -1<\alpha<1$

- $W_t=\ln Y_t$

---
# Transformaciones en modelos estacionales

**Ejemplo de turistas:**


.pull-left[
Sin transformación $(Y_t)$

```{r echo=FALSE, out.width = "70%", fig.align="center"}
turistas<-read.csv("turistas.csv",sep=";")
y<-ts(turistas$turistas,start=c(1991,1),frequency=12)
autoplot(y) 
```

]

.pull-right[

Con transformación $(\ln Y_t)$


```{r echo=FALSE, out.width = "70%", fig.align="center"}
autoplot(log(y))  
```

]

---
# Modelos de series estacionales

- La idea es utilizar un factor (variables indicadoras) como predictor para indicar el periodo de la estacionalidad.
- Ejemplo de turistas (datos mensuales y con tendencia cuadrática)

```{r echo=FALSE, out.width = "70%", fig.align="center"}
turistas<-read.csv("turistas.csv",sep=";")
y<-ts(turistas$turistas,start=c(1991,1),frequency=12)
autoplot(y) 
```

---
# Modelos de series estacionales

$$ln Y_t=\alpha_0+\alpha_1 t + \alpha_2 t^2 +\beta_1 I_{1}+...+\beta_11 I_{11}+\epsilon_t$$ 

```{r echo=FALSE, comment=""}
w<-log(y)

tiempo<-seq(1,length(y))
tiempo2<-tiempo^2
mes<-rep(seq(1,12),10)
mes<-as.factor(mes)

datos1<-data.frame(w,tiempo,tiempo2,mes)
mod<-lm(w~tiempo+tiempo2+mes,datos1)
round(summary(mod)$coefficients,4)
```

---
# Modelos de series estacionales mediante variables indicadoras

```{r echo=FALSE, out.width = "50%", fig.align="center"}
mod3<-tslm(w~trend+I(trend^2)+season,datos1)
pronostico<-forecast(mod3,h=12)

autoplot(w) +
  ylab("turistas") +
  autolayer(mod3$fitted.values, series = "ajustado") +
  autolayer(pronostico, series = "pronostico")
```






