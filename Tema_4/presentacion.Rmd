---
title: "Tema 2.3. Series de tiempo para pronóstico"
subtitle: "Curso: Decisiones Estadísticas y Administrativas"
author: "Prof. Shu Wei Chou Chen"
institute: "Escuela de Estadística, UCR"
date: ""
output:
  xaringan::moon_reader:
    css: [default,  metropolis, metropolis-fonts, ninjutsu,"text_color.css"]
  html_document: default
  pdf_document: default
fontsize: 12pt
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE,fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%")
```


```{r, include=FALSE}
library(ggplot2)
library(forecast)
library(fpp2)
library(astsa)
library(car)
library(astsa)
```

# Subtemas:

1. Series de tiempo y pronóstico
2. Descomposición de series temporales
3. Técnicas de suavizamiento exponencial
4. **Regresión aplicada a series cronológicas**

---
# Contenido

1. Introducción
2. Regresión lineal simple
3. Regresión lineal múltiple
2. Modelos de tendencia
3. Transformaciones en modelos estacionales
4. Modelos de series estacionales mediante variables indicadoras

---
# Introducción

- La idea es ajustar un modelo de regresión para la serie temporal $Y_t, t=1,...,T$ utlizando un conjunto de $p$ covariables: $X_1,...,X_p$.

- Por ejemplo: 
  - $Y$ una serie mensual de ventas con la variable independiente $X_1$ gasto mensual en anuncios.
  - $Y$ una serie diaria de demanda de energía eléctrica con las variables independientes: $X_1$ temperatura y $X_2$ el día de la semana.


---
# Regresión lineal simple

Un modelo de regresión lineal simple establece una relación lineal entre una variable dependiente $Y$ y una sola variable predictora $X$:

$$
  Y_t = \beta_0 + \beta_1 X_t + \epsilon_t,
$$
donde los coeficientes $\beta_0$ y $\beta_1$ denotan la intersección y la pendientes, respectivamente;  
$\varepsilon_t \overset{\text{iid}}{\sim} N(0,\sigma^2)$.

- La intercepción $\beta_0$ representa el valor predicho de $y$ cuando $X=0$.
- La pendiente $\beta_1$ representa el cambio promedio previsto en $Y$ resultante de un aumento de una unidad en $X$.


---
# Regresión lineal simple

```{r SLRpop1, out.width = "50%", fig.align="center", fig.cap="Ejemplo simulado de un modelo de regresión lineal simple (Fig 5.1 de Hyndman).", echo=FALSE, warning=FALSE, message=FALSE}
set.seed(2)
x <- runif(50, 0, 4)
df <- data.frame(x=x,
                 y=3 + 10*x + rnorm(50, 0, 10))
ggplot(df, aes(x, y)) +
  geom_point() +
  geom_abline(slope=10, intercept=3,
              col="#990000", size=0.3) +
  geom_label(x=.3, y=40, parse=TRUE, col="#990000",
             label=" beta[0] + beta[1] * x") +
  geom_segment(x=.3, y=36, xend=0, yend=4,
        arrow=arrow(length = unit(0.02, "npc")),
          size=0.2, col='#990000') +
  geom_label(x=1.5, y=55, parse=TRUE, col="#000099",
           label="y[t] == beta[0] + beta[1] * x[t] + epsilon[t]") +
  geom_segment(x=1.5, y=52, xend=df$x[19]-0.03, yend=df$y[19]+1.5,
               arrow=arrow(length = unit(0.02, "npc")),
                 size=0.2, col='#000099') +
  geom_segment(x=df$x[19], y=df$y[19],
               xend=df$x[19], yend=3+10*df$x[19],
               col="#009900", size=0.2,
               arrow=arrow(length=unit(0.02,"npc"), ends="both")) +
  geom_label(x=df$x[19]-0.07,
             y=(df$y[19]+ 3+10*df$x[19])/2,
             col="#009900", label="epsilon[t]",
             parse=TRUE)
```

---
# Regresión lineal simple: Ejemplo

Se tienen datos de cambios porcentuales trimestrales (tasas de crecimiento) del gasto de consumo personal real (Y) e ingresos disponibles(X), para EE.UU. desde 1970 a 2016.


```{r, out.height = '300px', out.width = '500px', fig.align="center", echo=FALSE, warning=FALSE, message=FALSE}
autoplot(uschange[,c("Consumption","Income")]) +
  ylab("% change") + xlab("Year")

```

---
# Regresión lineal simple: Ejemplo

```{r, out.width = "50%", fig.align="center", echo=FALSE, warning=FALSE, message=FALSE}
uschange %>%
  as.data.frame() %>%
  ggplot(aes(x=Income, y=Consumption)) +
    ylab("Consumption (quarterly % change)") +
    xlab("Income (quarterly % change)") +
    geom_point() +
    geom_smooth(method="lm", se=FALSE)
```

---
# Regresión lineal simple: Ejemplo

```{r tslmcons, echo=TRUE}
tslm(Consumption ~ Income, data=uschange)
```

$$\hat{Y}_t=0.55 + 0.28X_t.$$
El coeficiente de pendiente muestra que un aumento de una unidad en $X$(un aumento de 1  punto porcentual en el ingreso personal disponible) resulta en un promedio de 0.28 unidades de aumento en $Y$.


---
# Regresión lineal múltiple

- La forma general de un modelo de regresión lineal múltiple es:

$$Y_t=\beta_0+\beta_1 X_{t,1}+\beta_2 X_{t,2}+...+\beta_p X_{t,p}+\epsilon_t, t=1,...,T,$$ 

donde $Y_t$ es la variable a pronosticar y $X_1,...,X_p$ son los $p$ variables predictoras. Las variables predictoras pueden ser numéricas o categóricas (con el manejo apropiado de factores).   
Los coeficientes $\beta_1,...,\beta_p$ miden el efecto de cada predictor después de tener en cuenta los efectos de todos los demás predictores del modelo. Por lo tanto, los coeficientes miden los efectos marginales de las variables predictoras.


---
# Regresión lineal múltiple

- El modelo de regresión lineal múltiple en su forma matricial:

$$
Y=X \beta+\epsilon
$$
donde

$$Y=\left[ \begin{array}{c}Y_1 \\ \vdots \\Y_T \end{array}  \right],~~ X= \left(\begin{array}{ccccc} 1& X_{11}& X_{12} & ... & X_{1p}\\ 1 &  X_{21}& X_{22} & ... &X_{2p}\\ \vdots& \vdots & \ddots &\vdots& \vdots\\ 1&  X_{T1}& X_{T2} & ... &X_{Tp} \end{array}\right),$$
$$\beta=\left[ \begin{array}{c}\beta_0 \\ \vdots \\\beta_T \end{array}  \right],~~\epsilon=\left[ \begin{array}{c}\epsilon_1 \\ \vdots \\\epsilon_T \end{array}  \right].$$ 

---
# Regresión lineal múltiple


**Supocisiones del modelo:**

- La relación entre la variable de pronóstico y las variables predictoras satisface esta ecuación lineal.

- Los errores $\varepsilon_1,...,\varepsilon_T$:
  - tienen media cero,
  - no están autocorrelacionados,
  - no están relacionados con las variables predictoras

- Los errores se distribuyan normalmente con una varianza constante $\sigma^2$.

- Cada predictor $X_i, i=1,...,p$ supone que es observado y fijo, i.e. no es una variable aleatoria.

---
# Regresión lineal múltiple

**Tópicos importantes:**

1. Estimación:
  - por mínimos cuadrados.
  - por máxima verosimilitud.
  
2. Selección de variables

3. Diagnósticos

4. Medidas remediales

---
# Regresión lineal múltiple

- Estimación por mínimos cuadrados: minimizar
$$\sum_{t=1}^T \epsilon_t^2=\sum_{t=1}^T [y_t-(\beta_0+\beta_1 x_{1,t}+...+\beta_k x_{k,t})]^2,$$
en función de $\beta_0,...\beta_k$.

- Como resultado:
$$
\hat{\beta}=(X^\top X)^{-1}X^\top Y.
$$

- El estimador de máxima verosimilitud es equivalente (¿?)
---
# Regresión lineal múltiple: Ejemplo

- El objetivo es generar pronósticos del consumo más precisos usando otros predictores, además del ingreso personal. 

```{r MultiPredictors, out.width = "40%", fig.align="center", echo=FALSE, fig.cap="Variaciones porcentuales trimestrales en la producción industrial y ahorros personales y variaciones trimestrales en la tasa de desempleo de los EE. UU. Durante el período 1970T1-2016T3 (Hyndman)."}

autoplot(uschange[,3:5], facets = TRUE, colour=TRUE) +
  ylab("") + xlab("Year") +
  guides(colour="none")
```


---
# Regresión lineal múltiple: Ejemplo

```{r ScatterMatrix, out.width = "50%", fig.align="center", echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Matriz de diagrama de dispersión del gasto de consumo de EE. UU. y los cuatro predictores."}
uschange %>%
  as.data.frame() %>%
  GGally::ggpairs()
```


---
# Regresión lineal múltiple: Ejemplo

```{r tslm_mult, echo=FALSE}
fit.consMR <- tslm(
  Consumption ~ Income + Production + Unemployment + Savings,
  data=uschange)
summary(fit.consMR)
```


---
# Regresión lineal múltiple: Ejemplo

```{r, out.width = "60%", fig.align="center", echo=FALSE}
autoplot(uschange[,'Consumption'], series="Data") +
  forecast::autolayer(fitted(fit.consMR), series="Fitted") +
  xlab("Year") + ylab("") +
  ggtitle("Cambio porcentual de gastos de consumo en EE.UUU ") +
  guides(colour=guide_legend(title=" "))

```


---
# Regresión lineal múltiple: Ejemplo

```{r, out.width = "50%", fig.align="center", echo=FALSE}
checkresiduals(fit.consMR)

```


---
# Regresión espuria

- Cuando las series no son estacionarias, los resultados de regresión no son confiables y presenta lo que se llama correlación espuria.

- Los datos de las series cronológicas de tendencias pueden parecer relacionados.
- Por ejemplo, los pasajeros aéreos en Australia tienen una correlación positiva con la producción de arroz en Guinea.

```{r spurious, echo=FALSE, out.height = '250px', out.width = '350px', fig.align="center", fig.asp=0.6, warning=FALSE, message=FALSE}
aussies <- window(ausair, end=2011)
p1 <- autoplot(aussies) + xlab("Year") + ylab("Air Passengers in Australia")
p2 <- autoplot(guinearice) + xlab("Year") + ylab("Rice Production in Guinea")
p3 <- cbind(guinearice, aussies) %>%
  as.data.frame() %>%
  ggplot(aes(x=guinearice, y=aussies)) +
    geom_point() +
    ylab("Air Passengers in Australia (millions)") +
    xlab("Rice Production in Guinea (million tons)")
gridExtra::grid.arrange(gridExtra::arrangeGrob(p1, p2),p3, ncol = 2)
```

---
# Regresión espuria

```{r tslm_mult1, echo=FALSE}
fit <- tslm(aussies ~ guinearice)
summary(fit)
```

---
# Regresión espuria

```{r tslm_mult2, out.height = '300px', out.width = '300px',fig.align="center", echo=FALSE}
checkresiduals(fit)
```

---
# Modelos de tendencia

- Como las variables independiente son asumidas como fijas, se puede utilizar el tiempo como una variable independiente.

- Los modelos más frecuentes:
  - **Tendencia lineal**:
  $$Y_t=\beta_0+\beta_1 t + \epsilon_t$$
  - **Tendencia cuadrática**:
  $$Y_t=\beta_0+\beta_1 t +\beta_2 t^2 + \epsilon_t$$

- Regresión no lineal.
  - Por ejemplo: LOESS.

---
# Modelos de tendencia

Vamos a ajustar un modelo de tendencia cuadrática a la serie de graduados de ITCR de 1975 a 2002:
  $$Y_t=\beta_0+\beta_1 t +\beta_2 t^2 + \epsilon_t, t=1,...,T$$

```{r echo=FALSE, out.width = "45%", fig.align="center"}
itcrgrad<-read.csv("ITCR.csv",sep=",")
y<-ts(itcrgrad$graduados,start=1975)
tiempo<-seq(1,length(y))
tiempo2<-tiempo^2
mod<-lm(y~tiempo+tiempo2)

autoplot(y) +
  ylab("ITCR") +
  autolayer(ts(mod$fitted.values,start=1975), series = "ajustado") 

```


---

# Modelos de tendencia


```{r comment=NA, out.width = "35%", fig.align="center"}
tiempo<-seq(1,length(y))
tiempo2<-tiempo^2
mod<-lm(y~tiempo+tiempo2)
summary(mod)
```


---
# Modelos de tendencia

.pull-left[
```{r echo=FALSE}
hist(mod$residuals,main="",xlab="residuales")
```
]

.pull-right[
```{r echo=FALSE}
ts.plot(mod$residuals,ylab="residuales") 
```
]

---
# Modelos de tendencia

- Supuestos:
  - Normalidad
  - Homoscedasticidad
  - Autocorrelación en el tiempo

```{r echo=FALSE, out.width = "35%", fig.align="center"}
e<-mod$residuals
lag1.plot(e)
```


---
# Modelos de tendencia

**Prueba de Durbin-Watson:**
Dado un modelo de regresión ajustado, suponga que el tipo de autocorrelación entre los errores tiene orden 1, i.e.
$$\epsilon_t= \rho \epsilon_{t-1}+a_t$$
donde $\rho$ es la correlación de un rezago y $a_t \sim N(0,\sigma^2)$

Defina las hipótesis:<br /> 
$H_0: \rho=0$  <br /> 
$H_1: \rho \neq 0$ <br /> 


```{r comment=""}
durbinWatsonTest(mod)
```

---
# Modelos de tendencia


```{r, out.width = "50%", fig.align="center", echo=FALSE}
checkresiduals(mod)

```


---
# Transformaciones en modelos estacionales

- Si la magnitud del cambio estacional se mantiene aproximadamente constante con el cambio del nivel de la serie, se dice que la variación estacional es constante (aditiva).
- Si la variación estacional aumenta proporcionalmente con el nivel de la serie, se dice que la variación estacional es multiplicativa.

Transformaciones usadas en la práctica:

- $W_t=Y_t^\alpha,~~\text{con} -1<\alpha<1$

- $W_t=\ln Y_t$

---
# Transformaciones en modelos estacionales

**Ejemplo de turistas:**


.pull-left[
Sin transformación $(Y_t)$

```{r echo=FALSE, out.width = "70%", fig.align="center"}
turistas<-read.csv("turistas.csv",sep=";")
y<-ts(turistas$turistas,start=c(1991,1),frequency=12)
autoplot(y) 
```

]

.pull-right[

Con transformación $(\ln Y_t)$


```{r echo=FALSE, out.width = "70%", fig.align="center"}
autoplot(log(y))  
```

]

---
# Modelos de series estacionales mediante variables indicadoras

- La idea es utilizar un factor (variables indicadoras) como predictor para indicar el periodo de la estacionalidad.
- Ejemplo de turistas (datos mensuales y con tendencia cuadrática)

$$ln Y_t=\alpha_0+\alpha_1 t + \alpha_2 t^2 +\beta_1 I_{1}+...+\beta_11 I_{11}+\epsilon_t$$ 

```{r echo=FALSE, comment=""}
w<-log(y)

tiempo<-seq(1,length(y))
tiempo2<-tiempo^2
mes<-rep(seq(1,12),10)
mes<-as.factor(mes)

datos1<-data.frame(w,tiempo,tiempo2,mes)
mod<-lm(w~tiempo+tiempo2+mes,datos1)
round(summary(mod)$coefficients,4)
```

---
# Modelos de series estacionales mediante variables indicadoras

```{r echo=FALSE, out.width = "50%", fig.align="center"}
mod3<-tslm(w~trend+I(trend^2)+season,datos1)
pronostico<-forecast(mod3,h=12)

autoplot(w) +
  ylab("turistas") +
  autolayer(mod3$fitted.values, series = "ajustado") +
  autolayer(pronostico, series = "pronostico")
```



---
# Extensiones de modelos de regresión

- Modelos no lineales.

- Modelos lineales generalizados (GLM).
  - variables dependientes que pertenecen a una familia exponencial: Poisson, Exponencial, etc.
- Modelos aditivos generalizados (GAM).
  - efectos fijos y aleatorios.

- Modelos aditivos generalizados para locación, escala y forma (GAMLSS).


---
# Ejemplo

###Temperatura diaria (F) en Cairo de 01-01-1995 hasta 21-05-2005


```{r echo=FALSE, message=FALSE,warning=FALSE, out.width = "50%", fig.align="center"}
library(gamair)
data(cairo)
plot(cairo$temp~cairo$time,type="l", ylab="temp",xlab="time")
```



---
# Ejemplo

###Temperatura diaria (F) en Cairo de 01-01-1995 hasta 21-05-2005

- Ajuste de un GAM


```{r echo=FALSE, message=FALSE,warning=FALSE, out.width = "50%", fig.align="center"}
library(mgcv)
ctamm <- gamm(temp~s(day.of.year,bs="cc",k=20)+s(time,bs="cr"),
              data=cairo,correlation=corAR1(form=~1|year))
pred<-fitted(ctamm$gam)
plot(cairo$temp~cairo$time,type="l", ylab="temp",xlab="time")
points(cairo$time,pred,type="l",col=2,lwd=2)
```

---
# Ejemplo

###Temperatura diaria (F) en Cairo de 01-01-1995 hasta 21-05-2005

- Periodograma (análisis espectral)

```{r echo=FALSE, message=FALSE,warning=FALSE,out.width = "40%", fig.asp = .4, fig.align="center"}
library(astsa)
x<-cairo$temp
par(mfrow=c(1,2))
x.spec <-mvspec(x,log="no")
plot(x.spec)
frecuencia<-x.spec$freq[x.spec$spec==max(x.spec$spec)]
abline(v=frecuencia,col=2)
1/frecuencia
```

La frecuencia dominante $\omega$ es 0.002864583, es decir, se completa un ciclo aproximadamente cada 349.09 días.

---
# Ejemplo

###Temperatura diaria (F) en Cairo de 01-01-1995 hasta 21-05-2005

- Modelo de regresión lineal con covariables senos y cosenos
$$y_t=\alpha+\beta_1 cos(2 \pi \omega t)+\beta_2 sen(2 \pi \omega t)+\varepsilon_t$$

```{r echo=FALSE, message=FALSE,warning=FALSE,out.width = "40%", fig.align="center"}
cos1<-cos(2*pi*frecuencia*cairo$time)
sin1<-sin(2*pi*frecuencia*cairo$time)
data=data.frame(x=x,time=cairo$time,cos=cos1,sen=sin1)
mod<-lm(x~cos1+sin1,data=data)
plot(cairo$temp~cairo$time,type="l", ylab="temp",xlab="time")
points(cairo$time,fitted(mod),type="l",col=2,lwd=2)
```

---
# Próximo tema

### Tema 5: Modelos de series temporales


